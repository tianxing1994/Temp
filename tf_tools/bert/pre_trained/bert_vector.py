#!/usr/bin/python3
# -*- coding: utf-8 -*-
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import codecs
import collections
import json
import os
import sys
import re

pwd = os.path.abspath(os.path.dirname(__file__))
sys.path.append(pwd)

import numpy as np

import modeling
import tokenization
import tensorflow as tf


def get_flags():
    vocab_file = os.path.join(pwd, 'chinese_L-12_H-768_A-12/vocab.txt')
    bert_config_file = os.path.join(pwd, 'chinese_L-12_H-768_A-12/bert_config.json')
    init_checkpoint = os.path.join(pwd, 'chinese_L-12_H-768_A-12/bert_model.ckpt')

    tf.flags.DEFINE_string("layer_indexes", "-1,-2,-3,-4", "")

    tf.flags.DEFINE_string(
        "bert_config", bert_config_file,
        "The config json file corresponding to the pre-trained BERT model. "
        "This specifies the model architecture.")

    tf.flags.DEFINE_integer(
        "max_seq_length", 128,
        "The maximum total input sequence length after WordPiece tokenization. "
        "Sequences longer than this will be truncated, and sequences shorter "
        "than this will be padded.")

    tf.flags.DEFINE_string(
        "init_checkpoint", init_checkpoint,
        "Initial checkpoint (usually from a pre-trained BERT model).")

    tf.flags.DEFINE_string("vocab_file", vocab_file,
                        "The vocabulary file that the BERT model was trained on.")

    tf.flags.DEFINE_bool(
        "do_lower_case", True,
        "Whether to lower case the input text. Should be True for uncased "
        "models and False for cased models.")

    tf.flags.DEFINE_integer("batch_size", 32, "Batch size for predictions.")

    tf.flags.DEFINE_bool("use_tpu", False, "Whether to use TPU or GPU/CPU.")

    tf.flags.DEFINE_string("master", None,
                        "If using a TPU, the address of the master.")

    tf.flags.DEFINE_integer(
        "num_tpu_cores", 8,
        "Only used if `use_tpu` is True. Total number of TPU cores to use.")

    tf.flags.DEFINE_bool(
        "use_one_hot_embeddings", False,
        "If True, tf.one_hot will be used for embedding lookups, otherwise "
        "tf.nn.embedding_lookup will be used. On TPUs, this should be True "
        "since it is much faster.")

    FLAGS = tf.flags.FLAGS

    FLAGS.layer_indexes = [int(x) for x in FLAGS.layer_indexes.split(",")]
    FLAGS.bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config)
    return FLAGS


class InputExample(object):
    def __init__(self, unique_id, text_a, text_b):
        self.unique_id = unique_id
        self.text_a = text_a
        self.text_b = text_b


class InputFeatures(object):
    """A single set of features of data."""
    def __init__(self, unique_id, tokens, input_ids, input_mask, input_type_ids):
        self.unique_id = unique_id
        self.tokens = tokens
        self.input_ids = input_ids
        self.input_mask = input_mask
        self.input_type_ids = input_type_ids


class BertHelper(object):
    @staticmethod
    def _truncate_seq_pair(tokens_a, tokens_b, max_length):
        while True:
            total_length = len(tokens_a) + len(tokens_b)
            if total_length <= max_length:
                break
            if len(tokens_a) > len(tokens_b):
                tokens_a.pop()
            else:
                tokens_b.pop()

    @classmethod
    def convert_examples_to_features(cls, examples, seq_length, tokenizer):
        features = []
        for (ex_index, example) in enumerate(examples):
            tokens_a = tokenizer.tokenize(example.text_a)

            tokens_b = None
            if example.text_b:
                tokens_b = tokenizer.tokenize(example.text_b)

            if tokens_b:
                cls._truncate_seq_pair(tokens_a, tokens_b, seq_length - 3)
            else:
                if len(tokens_a) > seq_length - 2:
                    tokens_a = tokens_a[0:(seq_length - 2)]

            tokens = []
            input_type_ids = []
            tokens.append("[CLS]")
            input_type_ids.append(0)
            for token in tokens_a:
                tokens.append(token)
                input_type_ids.append(0)
            tokens.append("[SEP]")
            input_type_ids.append(0)

            if tokens_b:
                for token in tokens_b:
                    tokens.append(token)
                    input_type_ids.append(1)
                tokens.append("[SEP]")
                input_type_ids.append(1)

            input_ids = tokenizer.convert_tokens_to_ids(tokens)

            input_mask = [1] * len(input_ids)

            while len(input_ids) < seq_length:
                input_ids.append(0)
                input_mask.append(0)
                input_type_ids.append(0)

            assert len(input_ids) == seq_length
            assert len(input_mask) == seq_length
            assert len(input_type_ids) == seq_length

            if ex_index < 5:
                tf.logging.info("*** Example ***")
                tf.logging.info("unique_id: %s" % (example.unique_id))
                tf.logging.info("tokens: %s" % " ".join(
                    [tokenization.printable_text(x) for x in tokens]))
                tf.logging.info("input_ids: %s" % " ".join([str(x) for x in input_ids]))
                tf.logging.info("input_mask: %s" % " ".join([str(x) for x in input_mask]))
                tf.logging.info(
                    "input_type_ids: %s" % " ".join([str(x) for x in input_type_ids]))

            features.append(
                InputFeatures(
                    unique_id=example.unique_id,
                    tokens=tokens,
                    input_ids=input_ids,
                    input_mask=input_mask,
                    input_type_ids=input_type_ids))
        return features

    @staticmethod
    def model_fn_builder(bert_config, init_checkpoint, layer_indexes,
                         use_tpu, use_one_hot_embeddings):
        def model_fn(features, labels, mode, params):
            unique_ids = features["unique_ids"]
            input_ids = features["input_ids"]
            input_mask = features["input_mask"]
            input_type_ids = features["input_type_ids"]

            model = modeling.BertModel(
                config=bert_config,
                is_training=False,
                input_ids=input_ids,
                input_mask=input_mask,
                token_type_ids=input_type_ids,
                use_one_hot_embeddings=use_one_hot_embeddings
            )

            if mode != tf.estimator.ModeKeys.PREDICT:
                raise ValueError("Only PREDICT modes are supported: %s" % (mode))

            tvars = tf.trainable_variables()
            scaffold_fn = None
            (assignment_map, initialized_variable_names) = \
                modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)

            if use_tpu:
                def tpu_scaffold():
                    tf.train.init_from_checkpoint(init_checkpoint, assignment_map)
                    return tf.train.Scaffold()

                scaffold_fn = tpu_scaffold
            else:
                tf.train.init_from_checkpoint(init_checkpoint, assignment_map)

            tf.logging.info("**** Trainable Variables ****")
            for var in tvars:
                init_string = ""
                if var.name in initialized_variable_names:
                    init_string = ", *INIT_FROM_CKPT*"
                tf.logging.info("  name = %s, shape = %s%s", var.name, var.shape, init_string)

            all_layers = model.get_all_encoder_layers()

            predictions = {
                "unique_id": unique_ids,
            }

            for (i, layer_index) in enumerate(layer_indexes):
                predictions["layer_output_%d" % i] = all_layers[layer_index]

            output_spec = tf.contrib.tpu.TPUEstimatorSpec(
                mode=mode,
                predictions=predictions,
                scaffold_fn=scaffold_fn
            )
            return output_spec
        return model_fn

    @staticmethod
    def input_fn_builder(features, seq_length):
        all_unique_ids = []
        all_input_ids = []
        all_input_mask = []
        all_input_type_ids = []
        for feature in features:
            all_unique_ids.append(feature.unique_id)
            all_input_ids.append(feature.input_ids)
            all_input_mask.append(feature.input_mask)
            all_input_type_ids.append(feature.input_type_ids)

        def input_fn(params):
            batch_size = params["batch_size"]
            num_examples = len(features)
            d = tf.data.Dataset.from_tensor_slices({
                "unique_ids":
                    tf.constant(all_unique_ids, shape=[num_examples], dtype=tf.int32),
                "input_ids":
                    tf.constant(
                        all_input_ids, shape=[num_examples, seq_length],
                        dtype=tf.int32),
                "input_mask":
                    tf.constant(
                        all_input_mask,
                        shape=[num_examples, seq_length],
                        dtype=tf.int32),
                "input_type_ids":
                    tf.constant(
                        all_input_type_ids,
                        shape=[num_examples, seq_length],
                        dtype=tf.int32),
            })
            d = d.batch(batch_size=batch_size, drop_remainder=False)
            return d
        return input_fn

    @staticmethod
    def build_input_examples_from_sentences(sentences):
        examples = []
        unique_id = 0
        for sentence in sentences:
            line = tokenization.convert_to_unicode(sentence)
            if not line:
                break
            line = line.strip()
            text_a = None
            text_b = None
            m = re.match(r"^(.*) \|\|\| (.*)$", line)
            if m is None:
                text_a = line
            else:
                text_a = m.group(1)
                text_b = m.group(2)
            examples.append(
                InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b))
            unique_id += 1
        return examples


class BertVector(BertHelper):
    def __init__(self, batch_size, max_seq_length, vocab_file, bert_config, init_checkpoint, layer_indexes, do_lower_case, use_one_hot_embeddings, use_tpu, num_tpu_cores, master=None):
        self._batch_size = batch_size
        self._max_seq_length = max_seq_length
        self._vocab_file = vocab_file

        self._bert_config = bert_config
        self._init_checkpoint = init_checkpoint
        self._layer_indexes = layer_indexes
        self._do_lower_case = do_lower_case

        self._use_one_hot_embeddings = use_one_hot_embeddings
        self._use_tpu = use_tpu
        self._num_tpu_cores = num_tpu_cores
        self._master = master

        self._estimator = self._get_estimator()
        self._tokenizer = self._get_tokenizer()

    def _get_estimator(self):
        model_fn = self.model_fn_builder(
            bert_config=self._bert_config,
            init_checkpoint=self._init_checkpoint,
            layer_indexes=self._layer_indexes,
            use_tpu=self._use_tpu,
            use_one_hot_embeddings=self._use_one_hot_embeddings
        )

        is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2

        run_config = tf.contrib.tpu.RunConfig(
            master=self._master,
            tpu_config=tf.contrib.tpu.TPUConfig(
                num_shards=self._num_tpu_cores,
                per_host_input_for_training=is_per_host
            )
        )
        estimator = tf.contrib.tpu.TPUEstimator(
            use_tpu=self._use_tpu,
            model_fn=model_fn,
            config=run_config,
            predict_batch_size=self._batch_size
        )
        return estimator

    def _get_tokenizer(self):
        tokenizer = tokenization.FullTokenizer(
            vocab_file=self._vocab_file,
            do_lower_case=self._do_lower_case
        )
        return tokenizer

    def convert_sentences_to_vectors(self, sentences):
        examples = self.build_input_examples_from_sentences(sentences)
        features = self.convert_examples_to_features(
            examples=examples,
            seq_length=self._max_seq_length,
            tokenizer=self._tokenizer
        )

        input_fn = self.input_fn_builder(
            features=features, seq_length=self._max_seq_length
        )

        y_pred_iterator = self._estimator.predict(input_fn, yield_single_examples=False)
        result = None
        for y_pred in y_pred_iterator:
            vectors = y_pred['layer_output_2']
            result = vectors

        result = np.mean(result, axis=-2)
        return result


def demo1():
    FLAGS = get_flags()

    bert_vector = BertVector(
        batch_size=FLAGS.batch_size,
        max_seq_length=FLAGS.max_seq_length,
        vocab_file=FLAGS.vocab_file,
        bert_config=FLAGS.bert_config,
        init_checkpoint=FLAGS.init_checkpoint,
        layer_indexes=FLAGS.layer_indexes,
        do_lower_case=FLAGS.do_lower_case,
        use_one_hot_embeddings=FLAGS.use_one_hot_embeddings,
        use_tpu=FLAGS.use_tpu,
        num_tpu_cores=FLAGS.num_tpu_cores,
        master=FLAGS.master
    )
    sentences = ['华为 p40 pro 多少钱?', '荣耀 mate10']

    ret = bert_vector.convert_sentences_to_vectors(sentences=sentences)
    print(ret)
    return


if __name__ == '__main__':
    demo1()
